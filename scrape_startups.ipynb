{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "060b03c5",
   "metadata": {},
   "source": [
    "Install and import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eae9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lxml\n",
    "!pip install serpapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9d2c0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from urllib.parse import urlencode, urlunparse\n",
    "from urllib.request import urlopen, Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a9f012",
   "metadata": {},
   "source": [
    "Load stratups JSON data into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b1c2c2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('') as f: #startups_json.txt\n",
    "    json_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "22993fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json_data['data']\n",
    "startups_list = data['startups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda36e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in startups_list:\n",
    "    for val in obj:\n",
    "        #print(obj[val])\n",
    "        if(str(val)!='website'):\n",
    "            #print(str(val))\n",
    "            obj[val] = re.sub('<[^<]+?>', '', str(obj[val]))\n",
    "        elif(str(val)=='website'):\n",
    "            #obj[val] = re.sub('\\\\\\\\', '', str(obj[val]))\n",
    "            #print(obj[val])\n",
    "            soup = BeautifulSoup(str(obj[val]),\"html.parser\")\n",
    "            tag = soup.find('a', href=True)\n",
    "            obj[val] = str(tag['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "c96cff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(startups_list, orient='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a36805",
   "metadata": {},
   "source": [
    "Functions to search and extract urls based on company name and website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5911a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "                  \"(KHTML, like Gecko) Chrome/111.0.5563.65 Safari/537.36\"}\n",
    "\n",
    "def extract_elements(html_text, elm='p'):\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    paragraphs = []\n",
    "    for p in soup.find_all(elm):\n",
    "        paragraphs.append(p.text.strip())\n",
    "    return paragraphs\n",
    "\n",
    "def google_search_alt(q: str):\n",
    "    time.sleep(3)\n",
    "    params = {'q': q}\n",
    "    #html = requests.get('https://www.google.com/search', headers=headers, params=params).text\n",
    "    html = requests.get('https://www.bing.com/search', headers=headers, params=params).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    elms = extract_elements(str(soup), elm='cite')\n",
    "    #print(elms)\n",
    "    return elms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ee577f",
   "metadata": {},
   "source": [
    "Form search query term and add to a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "550216ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_website_list = []\n",
    "for index in range(len(df)):\n",
    "    name_website_list.append(df['startup'].iloc[index] +' '+ df['website'].iloc[index]+' linkedin about')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0edc032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['name_website_list'] = name_website_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea328e8",
   "metadata": {},
   "source": [
    "Loop through dataframe and extract url list from the search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a89d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_list = []\n",
    "i = 0\n",
    "for index in range(len(df)):\n",
    "    i = i +1\n",
    "    website = df['name_website_list'].iloc[index]\n",
    "    url = google_search_alt(q=website)\n",
    "    if(len(url)>0):\n",
    "        linkedin_list.append(url)\n",
    "    else:\n",
    "        linkedin_list.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f6ca9c",
   "metadata": {},
   "source": [
    "Create a new column with the extracted data and save file as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "51f76188",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['linkedin_list'] = linkedin_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "4efcffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('linked_in_scrape_v2', sep='\\t', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
