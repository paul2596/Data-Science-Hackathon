{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1f3c60d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d9b1b4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe to store extracted values\n",
    "df = pd.DataFrame(columns=['id','name', 'summary', 'followers', 'employees_on_linkedin', 'overview', 'industry', 'company Size', 'headquarters', 'founded', 'specialties', 'location','website'])\n",
    "\n",
    "# html files folder\n",
    "folder_path = '' #folder containing html files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a4f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_list = os.listdir(folder_path)\n",
    "# loop through all the files in the folder\n",
    "for filename in dir_list:\n",
    "    print(filename)\n",
    "    if filename.endswith('.html'):\n",
    "        # load html into a soup obj\n",
    "        with open(os.path.join(folder_path, filename), 'r',encoding=\"utf8\") as f:\n",
    "            soup = BeautifulSoup(f, 'html.parser')\n",
    "            main_div = soup.select_one('.org-top-card__primary-content')\n",
    "            \n",
    "            name = ''\n",
    "            if main_div:\n",
    "                name = main_div.select_one('h1').text.strip()\n",
    "                \n",
    "            summary = soup.select_one('.org-top-card-summary__tagline')  \n",
    "            if summary:\n",
    "                summary = summary.text.strip()\n",
    "            if main_div:\n",
    "                top_card_location = main_div.select_one('div.org-top-card-summary-info-list__info-item:nth-of-type(1)')\n",
    "                top_card_follower_count = main_div.select_one('div.org-top-card-summary-info-list__info-item:nth-of-type(2)')\n",
    "\n",
    "                if top_card_location:\n",
    "                    location = top_card_location.text.strip()\n",
    "\n",
    "                if top_card_follower_count:\n",
    "                    follower_count = top_card_follower_count.text.strip()\n",
    "                    follower_count = ''.join(filter(str.isdigit, follower_count))\n",
    "\n",
    "                #follower_count = main_div.select_one('div.org-top-card-summary-info-list__info-item:nth-of-type(3)').text.strip()\n",
    "                employee_count = 0\n",
    "                employee_count_content = soup.select_one('div.org-top-card-secondary-content__see-all-independent-link')\n",
    "\n",
    "                if employee_count_content:\n",
    "                    employee_anchor = employee_count_content.find('a')\n",
    "                    print(employee_anchor)\n",
    "                    if employee_anchor:\n",
    "                        employee_span = employee_count_content.find('span')\n",
    "                        if employee_span:\n",
    "                            employee_count = employee_span.text.strip()\n",
    "                            employee_count = ''.join(filter(str.isdigit, employee_count))\n",
    "                            print(employee_count)\n",
    "                    \n",
    "            print(follower_count)\n",
    "            # extract from overview\n",
    "            overview_content = soup.select_one('.org-grid__content-height-enforcer')\n",
    "            \n",
    "            overview,website,industry,headquarters,founded,location = '','','','','',''\n",
    "            company_size = 0\n",
    "            \n",
    "            #loop about us data list through content and add to a key value pair \n",
    "            if overview_content:\n",
    "                overview_items = overview_content.find('dl')\n",
    "                overview_dict = {}\n",
    "                \n",
    "                overview_items = overview_content.select_one('dl')\n",
    "                if overview_items:\n",
    "                    items = overview_items.select('dt, dd')\n",
    "                    \n",
    "                    for i in range(len(items)):\n",
    "                        if items[i].name == 'dt':\n",
    "                            key = items[i].text.strip()\n",
    "                            value = ''\n",
    "                            if i + 1 < len(items) and items[i + 1].name == 'dd':\n",
    "                                value = items[i + 1].text.strip()\n",
    "                            overview_dict[key] = value\n",
    "                    print(overview_dict)\n",
    "                \n",
    "                #null and empty string checks\n",
    "                if(overview_dict.__contains__('Industry')):\n",
    "                    industry = overview_dict.get('Industry', '')  \n",
    "                if(overview_dict.__contains__('Company size')):\n",
    "                    company_size = overview_dict.get('Company size', '')  \n",
    "                    #company_size = ''.join(filter(str.isdigit, company_size))\n",
    "                if(overview_dict.__contains__('Headquarters')):\n",
    "                    headquarters = overview_dict.get('Headquarters', '')  \n",
    "                if(overview_dict.__contains__('Founded')):\n",
    "                    founded = overview_dict.get('Founded', '')  \n",
    "                if(overview_dict.__contains__('Specialties')):\n",
    "                    specialties = overview_dict.get('Specialties', '')    \n",
    "                if(overview_dict.__contains__('Website')):\n",
    "                    website = overview_dict.get('Website', '')    \n",
    "                \n",
    "                location_card = soup.select_one('.org-location-card')\n",
    "                if location_card:\n",
    "                    primary_loc = location_card.find('p')\n",
    "                    if primary_loc:\n",
    "                        location = primary_loc.text.strip()\n",
    "                        \n",
    "                        \n",
    "                org_grid_content = soup.find('div', {'class': 'org-grid__content-height-enforcer'})\n",
    "                if org_grid_content:\n",
    "                    section = org_grid_content.find('section')\n",
    "                    overview_content = section.find('p')\n",
    "                    if overview_content:\n",
    "                        overview = overview_content.text.strip()\n",
    "                    \n",
    "                id = filename.replace(\".html\", \"\")    \n",
    "                # append extracted data to dataframe\n",
    "                df = df.append({'id':id,'name': name, 'summary': summary, 'followers': follower_count, 'employees_on_linkedin': employee_count, 'overview': overview, 'industry': industry, 'company Size': company_size, 'headquarters': headquarters, 'founded': founded, 'specialties': specialties, 'location': location,'website': website}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b0a557f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_excel('') #linked_in_link_scrape_v1.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d20f755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.rename(columns={'Unnamed: 0': 'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b2dbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp[temp['linked_in_link'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4c52882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['id'] = df['id'].astype(np.int64)\n",
    "final_df = pd.merge(df,temp[['id','startup','website','linked_in_link']],on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "91d4350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('linkedin_about_data_v1.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "880e1d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_final = final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a6e3ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tldextract\n",
    "\n",
    "def extract_domain(url):\n",
    "    if(url):\n",
    "        if isinstance(url, str):\n",
    "            ext = tldextract.extract(url)\n",
    "            return f\"{ext.domain}.{ext.suffix}\"\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def ignore_subdomain_and_compare(url):\n",
    "    if(url):\n",
    "        if isinstance(url, str):\n",
    "            ext = tldextract.extract(url)\n",
    "            if ext.subdomain:\n",
    "                domain = f\"{ext.domain}.{ext.suffix}\"\n",
    "            else:\n",
    "                domain = ext.registered_domain\n",
    "            return domain.lower().split(\".\")[0]\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "temp_final['domain1'] = temp_final['website_x'].apply(ignore_subdomain_and_compare)\n",
    "temp_final['domain2'] = temp_final['website_y'].apply(ignore_subdomain_and_compare)\n",
    "\n",
    "temp_final['same_domain'] = (temp_final['domain1'] == temp_final['domain2']).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "b1cf02ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_words(row):\n",
    "    words1 = row['name'].split()\n",
    "    words2 = row['startup'].split()\n",
    "    \n",
    "    return any(word.lower() in [w.lower() for w in words2] for word in words1)\n",
    "\n",
    "temp_final['match_company_name'] = temp_final.apply(match_words, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "8c381ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_final['company_or_domain_match'] = (temp_final['same_domain'] | temp_final['match_company_name']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ce2d0e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_final['company_and_domain_match'] = (temp_final['same_domain'] & temp_final['match_company_name']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a92c29e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_final['is_greece'] = temp_final['location'].str.contains('GR').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "3820f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_final['is_greece_domain'] = temp_final['website_x'].str.contains('.gr').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "549b7e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_final.to_csv('linkedin_about_data_w_domain_name_check_v1.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "48e4f0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "448"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_final['company_and_domain_match'].value_counts()[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
